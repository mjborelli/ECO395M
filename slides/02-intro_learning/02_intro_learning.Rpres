Principles of statistical learning
========================================================
author: ECO 395M  
date: James Scott (UT-Austin)
autosize: true
font-family: 'Gill Sans'
transition: none

Reference: Introduction to Statistical Learning, Chapters 1-2

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

```{r setup, echo=FALSE, display=FALSE}
opts_chunk$set(cache=TRUE)
```

Outline
========

1. Introduction  
2. Parametric vs nonparametric models: KNN    
3. Measuring accuracy  
4. Out-of-sample predictions  
5. Train/test splits  
6. Bias-variance tradeoff  
7. Intro to classification  




Introduction to predictive modeling  
========

The goal is to predict a target variable ($y$) with feature variables ($x$).  
- Zillow: predict price ($y$) using a house's features ($x$ = size, beds, baths, age, ...)  
- Citadel: predict next month's S&P ($y$) using this month's economic indicators ($x$ = unemployment, GDP growth rate, inflation, ...)  
- MD Anderson: predict a patient's disease progression ($y$) using his or her clinical, demographic, and genetic indicators ($x$)  
- Etc.

In data mining/ML/AI, this is called "supervised learning."  We've already seen a simple example (OLS with one $x$ feature)


Introduction to predictive modeling   
========

A useful way to frame this problem is to think that $y$ and $x$ are related like this:

$$
y_i = f(x_i) + e_i  
$$

where:
- $y_i$ is a scalar _outcome_ or _target_ variable  
- $x_i = (x_{i1}, x_{i2}, ... x_{iP})$ is a vector of features
- $f$ is an unknown function    

Our main purpose is to _learn_ $f(x)$ from the observed data.  


Example: predicting electricity demand
========


```{r, out.width = "800px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/ERCOTOperator_2.jpg")
```

Example: predicting electricity demand
========

ERCOT operates the electricity grid for 75% of Texas.

The 8 ERCOT regions are shown at right.  

***

```{r, out.width = "400px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/ercot_regions.png")
```


Example: predicting electricity demand
========

We'll focus on a basic prediction task:
- $y$ = demand (megawatts) in the Coast region at 3 PM, every day from 2010-2016.  
- $x$ = average daily tempature measured at Houston's Hobby Airport (degrees Celsius)
- Date sources: scraped from the ERCOT website and the National Weather Service  

***

```{r, out.width = "400px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/ercot_regions.png")
```


Demand versus temperature
========================================================
class: small-code

```{r, message=FALSE, echo=FALSE}
library(tidyverse)
theme_set(theme_bw(base_size=18))
loadhou = read.csv('loadhou.csv')
```

```{r, fig.width = 9, fig.height = 6, fig.align='center'}
ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  ylim(7000, 20000)
```


A linear model?
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
lm1 = lm(COAST ~ KHOU, data=loadhou)
lm1_pred = function(x) {
  predict(lm1, newdata=data.frame(KHOU=x))
}
ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  stat_function(fun=lm1_pred, color='red', size=2) +
  theme_bw(base_size=18) + 
  ylim(7000, 20000)
```

$$
f(x) = \beta_0 + \beta_1 x
$$


A quadratic model?
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
lm2 = lm(COAST ~ poly(KHOU, 2), data=loadhou)
lm2_pred = function(x) {
  predict(lm2, newdata=data.frame(KHOU=x))
}
ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  stat_function(fun=lm2_pred, color='red', size=2) +
  theme_bw(base_size=18) + 
  ylim(7000, 20000)
```

$$
f(x) = \beta_0 + \beta_1 x + \beta_2 x^2
$$

How about this model?
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
library(caret) 
library(foreach)
ytrain = loadhou$COAST
Xtrain = data.frame(KHOU=jitter(loadhou$KHOU))

knn50 = knnreg(Xtrain, ytrain, k=50)
knn50_pred = function(x) {
  predict(knn50, newdata=data.frame(KHOU=x))
}

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  stat_function(fun=knn50_pred, color='red', size=1.5, n=1001) +
  theme_bw(base_size=18) + 
  ylim(7000, 20000)
```

We can't write down an equation for this $f(x)$.  But we can define it by its behavior!
- If $x = 15$, what is the prediction for $y$?
- What about if $x = 30$?  

How do we estimate f?
======

Our _training data_ consists of pairs
$$
D_{\mbox{tr}} = \{(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)\}
$$

We then use some statistical method to estimate $f(x)$.  Here "statistical" just means "we apply some recipe to the data."

There are two general families of strategy.
- Parametric models: assume a particular, restricted functional form (e.g. linear, quadratic, logs, exp)
- nonparametric models: flexible forms not easily described by simple math functions

A quick comparison
======

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  stat_function(fun=lm1_pred, color='red', size=2) +
  theme_bw(base_size=18)
```

Parametric:

$f(x) = \beta_0 + \beta_1 x$

***

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  stat_function(fun=knn50_pred, color='red', size=1.5, n=1001) +
  theme_bw(base_size=18)
```

Nonparametric (k-nearest neighbors):

$f(x)$ = average $y$ value of the 50 points closest to $x$


Estimating a parametric model: three steps
======



1. Choose a functional form of the model, e.g.
$$
f(x) = \beta_0 + \beta_1 x
$$

2. Choose a _loss function_ that measures the difference between the model predictions $f(x)$ and the actual outcomes $y$.  E.g. least squares:
$$
\begin{aligned}
L(f) &= \sum_{i=1}^N (y_i - f(x_i))^2 \\
&= \sum_{i=1}^N (y_i - \beta_0 - \beta_1 x_i)^2
\end{aligned}
$$

3. Find the parameters that minimize the loss function.  


Estimating k-nearest neighbors
======

Suppose we have our training data in the form of $(x_i, y_i)$ pairs.

Now we want to predict $y$ at some new point $x^{\star}$.

1. Pick the $K$ points in the training data whose $x_i$ values are closest to $x^{\star}$.  
2. Average the $y_i$ values for those points and use this average to estimate $f(x^{\star})$.  

There are no parameters (i.e. the $\beta$'s in a linear model) to estimate.  Rather, the estimate for $f(x)$ is defined by a particular _algorithm_ applied to the data set.  (Note for the mathematically rigorous: $f(x)$ is defined _point-wise_, that is, by applying the same recipe at any point $x$.)

At x=5
======


```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
K = 50

x_star = 5
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

At x=10
======


```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
K = 50

x_star = 10
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

At x=15
======


```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
K = 50

x_star = 15
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

At x=20
======


```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
K = 50

x_star = 20
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

At x=25
======


```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
K = 50

x_star = 25
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```


At x=30
======


```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
K = 50

x_star = 30
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```

At x=35
======


```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
K = 50

x_star = 35
delta = abs(x_star - loadhou$KHOU)
o1 = order(delta)
nn = head(o1, K)
loadhou_nn = loadhou[nn,]

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  geom_vline(xintercept=x_star) + 
  geom_point(data = loadhou_nn, mapping = aes(x = KHOU, y = COAST), color='blue') + 
  geom_point(data = data.frame(x=x_star, y = mean(loadhou_nn$COAST)),
             mapping = aes(x = x, y = y), color='red', size=3) + 
  ylim(7000, 20000)
```


The predictions across all x values
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}

ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  stat_function(fun=knn50_pred, color='red', size=1.5, n=1001) +
  theme_bw(base_size=18) + 
  ylim(7000, 20000)
```


Two questions
========================================================

So why average the nearest $K=50$ neighbors?  Why not $K=2$, or $K=200$?

And if we're free to pick any value of $K$ we like, how should we choose?


K=2
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=2)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}

p_base = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```

K=5
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=5)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}


p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```


K=10
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=10)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}


p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```


K=20
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=20)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}

p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```


K=50
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=50)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}


p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```

K=100
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=100)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}


p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```

K=200
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knnreg(Xtrain, ytrain, k=200)
knn_pred = function(x) {
  predict(knn_model, newdata=data.frame(KHOU=x))
}



p_base + stat_function(fun=knn_pred, color='red', size=1.5, n=1001)
```

K=500
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
library(FNN)
Xtest = data.frame(KHOU = seq(-2, 36, by=0.1))

knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 500)
d_test = data.frame(Xtest, ypred = knn_model$pred)
p_base = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

K=1000
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 1000)
d_test = data.frame(Xtest, ypred = knn_model$pred)
p_base = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

K=1500
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 1500)
d_test = data.frame(Xtest, ypred = knn_model$pred)
p_base = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

K=2000
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 2000)
d_test = data.frame(Xtest, ypred = knn_model$pred)
p_base = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

K=2357
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 2357)
d_test = data.frame(Xtest, ypred = knn_model$pred)
p_base = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```


Complexity, generalization, and interpretion
=====

As we have seen in the examples above, there are lots of options in estimating $f(x)$.

Some methods are very flexible, and some are not... why might we ever choose a less flexible model?  

1. Simple, more restrictive methods are usually easier to interpret  

2. More importantly, it is often the case that simple models make _more accurate predictions_ than very complex ones.  


Measuring accuracy
=====

Let's turn to a deceptively subtle question: how accurate is each of these models?  

A standard measure of accuracy is the root mean-squared error, or RMSE:  
$$
\mathrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - f(x_i))^2 }
$$

This measures, on average, how large are the "mistakes" (errors) made by the model on the training data.  (OLS minimizes this quantity.)


Measuring accuracy: linear vs. quadratric
=====

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
lm1 = lm(COAST ~ KHOU, data=loadhou)
lm2 = lm(COAST ~ poly(KHOU, 2), data=loadhou)
rmse = function(y, ypred) {
  sqrt(mean((y-ypred)^2))
}

rmse_lm1 = rmse(ytrain, predict(lm1))
rmse_lm2 = rmse(ytrain, predict(lm2))

d_test1 = data.frame(Xtest, ypred = predict(lm1, Xtest))
d_test2 = data.frame(Xtest, ypred = predict(lm2, Xtest))

p_base + geom_path(data=d_test1, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

$$
RMSE = 1807
$$

***
```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
p_base + geom_path(data=d_test2, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

$$
RMSE = 1001
$$



Measuring accuracy: RMSE versus K
=====

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
k_grid = unique(round(exp(seq(log(2000), log(2), length=100))))
rmse_grid_in = foreach(k = k_grid, .combine='c') %do% {
  knn_model = knn.reg(Xtrain, Xtrain, ytrain, k = k)
  rmse(ytrain, knn_model$pred)
}

revlog_trans <- function(base = exp(1)) {
  require(scales)
    ## Define the desired transformation.
    trans <- function(x){
                 -log(x, base)
                }
    ## Define the reverse of the desired transformation
    inv <- function(x){
                 base^(-x)
                }
    ## Creates the transformation
    scales::trans_new(paste("revlog-", base, sep = ""),
              trans,
              inv,  ## The reverse of the transformation
              log_breaks(base = base), ## default way to define the scale breaks
              domain = c(1e-100, Inf) 
             )
    }

rmse_grid_in = data.frame(K = k_grid, RMSE = rmse_grid_in)
ggplot(data=rmse_grid_in) + 
  geom_path(aes(x=K, y=RMSE)) + 
  scale_x_continuous(trans=revlog_trans(base = 10)) + 
  geom_hline(yintercept=1001, color='blue', size=1) + 
  geom_hline(yintercept=1807, color='red', size=1)
```


So we should pick K=2, right?
========================================================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knn.reg(Xtrain, Xtest, ytrain, k = 2)
d_test = data.frame(Xtest, ypred = knn_model$pred)
p_base = ggplot(data = loadhou) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)

p_base + geom_path(data=d_test, mapping = aes(x = KHOU, y = ypred), color='red', size=1.5)
```

$$
\mathrm{RMSE} = 669
$$


So we should pick K=2? Ask Yogi!
=========

```{r, out.width = "800px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/berra0.jpg")
```


So we should pick K=2? Ask Yogi!
=========

```{r, out.width = "800px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/berra1.jpg")
```

So we should pick K=2? Ask Yogi!
=========

```{r, out.width = "800px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/berra2.jpg")
```

So we should pick K=2? Ask Yogi!
=========

```{r, out.width = "800px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/berra3.jpg")
```

So we should pick K=2? Ask Yogi!
=========

```{r, out.width = "800px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/berra4.jpg")
```


Out-of-sample accuracy
=========

Make good predictions about the past isn't very impressive.  

Our very complex (K=2) model earned a low RMSE by simply memorizing the random pattern of noise in the training data.

It's like getting a perfect score on the GRE when someone tells you what the questions are ahead of time: it doesn't predict anything about how well you'll do in the future.  


Out-of-sample accuracy
=========

Key idea: what really matters is our prediction accuracy out-of-sample!!!

Suppose we have $M$ additional observations $(x_i^{\star}, y_i^{\star})$ for $i = 1, \ldots, M$, which we _did not use_ to fit the model.  We'll call this the "testing" data, to distinguish it from our original ("training") data.  

What really matters is the model's out-of-sample root mean-squared error:
$$
\mathrm{RMSE}_{\mathrm{out}} = \sqrt{ \frac{1}{M} \sum_{i=1}^M (y_i^{\star} - f(x_i^{\star}))^2 }
$$



Using a train/test split
=========

We don't have any "future data" to use to test our model.  We just have our $N$ original data points.  So we have to fake it by splitting our data set $D$ into two subsets:
- A training set $D_{in}$ of size $N_{in} < N$, to use for fitting the models under consideration.  
- A testing set $D_{out}$ of size $N_{out}$.
- $D = D_{in} \cup D_{out}$ and $N = N_{in} + N_{out}$, but $D_{in} \cap D_{out} = \emptyset$.  No cheating!

We use $D_{in}$ _only_ to fit the models, and $D_{out}$ _only_ to compare the out-of-sample predictive performance of the models.  


On our ERCOT data
===================


```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
N = nrow(loadhou)
N_train = floor(0.8*N)
train_ind = sort(sample.int(N, N_train, replace=FALSE))

D_all = loadhou; D_all$set = 'test'; D_all$set[train_ind] = 'train'
D_train = loadhou[train_ind,]
D_test = loadhou[-train_ind,]


p_split = ggplot(data = D_all) + 
  geom_point(mapping = aes(x = KHOU, y = COAST, color=set), alpha=0.8) + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)
  
p_split
```

Linear model: train
===================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)
  
# Fit the model
lm1 = lm(COAST ~ KHOU, data=D_train)

# Plot
D_plot = data.frame(KHOU = seq(-2, 36, by=0.1))
D_plot$COAST = predict(lm1, D_plot)

p_train + geom_path(data=D_plot, mapping = aes(x=KHOU, y=COAST), color='red', size=1.5)
```


Linear model: test
===================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
p_test = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000)
  
# Predict
y_pred = predict(lm1, D_test)
lm1_rmse = rmse(D_test$COAST, y_pred)

# Plot
D_plot = data.frame(KHOU = seq(-2, 36, by=0.1))
D_plot$COAST = predict(lm1, D_plot)

p_test + geom_path(data=D_plot, mapping = aes(x=KHOU, y=COAST), color='red', size=1.5)
```

$$
RMSE_{out} = `r round(lm1_rmse, 0)`
$$



Quadratic model: train
===================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
# Fit the model
lm2 = lm(COAST ~ poly(KHOU,2), data=D_train)

# Plot
D_plot = data.frame(KHOU = seq(-2, 36, by=0.1))
D_plot$COAST = predict(lm2, D_plot)

p_train + geom_path(data=D_plot, mapping = aes(x=KHOU, y=COAST), color='red', size=1.5)
```


Quadratic model: test
===================

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
# Predict
y_pred = predict(lm2, D_test)
lm2_rmse = rmse(D_test$COAST, y_pred)

# Plot
p_test + geom_path(data=D_plot, mapping = aes(x=KHOU, y=COAST), color='red', size=1.5)
```

$$
RMSE_{out} = `r round(lm2_rmse, 0)`
$$


K-nearest neighbors: test  
=======

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
D_test = arrange(D_test, KHOU)
y_train = D_train$COAST
y_test = D_test$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))
X_test = data.frame(KHOU=D_test$KHOU)

k_grid = unique(round(exp(seq(log(1500), log(2), length=100))))
rmse_grid_out = foreach(k = k_grid, .combine='c') %do% {
  knn_model = knn.reg(X_train, X_test, y_train, k = k)
  rmse(y_test, knn_model$pred)
}

rmse_grid_out = data.frame(K = k_grid, RMSE = rmse_grid_out)

p_out = ggplot(data=rmse_grid_out) + 
  geom_path(aes(x=K, y=RMSE, color='testset'), size=1.5) + 
  scale_x_continuous(trans=revlog_trans(base = 10)) + 
  geom_hline(yintercept=lm2_rmse, color='blue', size=1) + 
  geom_hline(yintercept=lm1_rmse, color='red', size=1)

ind_best = which.min(rmse_grid_out$RMSE)
k_best = k_grid[ind_best]

p_out + geom_path(data=rmse_grid_in, aes(x=K, y=RMSE, color='trainset'),size=1.5) + 
  ylim(700, 2000) +
  scale_colour_manual(name="RMSE",
    values=c(testset="black", trainset="grey")) + 
  geom_vline(xintercept=k_best, color='darkgreen', size=1.5)
```

Not too simple, not too complex... This plot illustrates the bias-variance trade-off, one of the key ideas of this course.

K-nearest neighbors: at the optimal k  
=======
```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
knn_model = knn.reg(X_train, X_test, y_train, k = k_best)
rmse_best = rmse(y_test, knn_model$pred)

D_test$ypred = knn_model$pred
p_test + geom_path(data=D_test, mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

$$
\mathrm{RMSE}_{out} = `r round(rmse_best, 0)`
$$



Take-home lessons
=========


- In general, $RMSE_{out} > RMSE_{in}$.  That is, the estimate of RMSE from the training set is an over-optimistic assessment of how big your errors will be for future data.  
- For very complex models, $RMSE_{in}$ can be _wildly_ optimistic.  
- The best model is usually one that balances simplicity with explanatory power.  
- Estimating $RMSE_{out}$ using a train-test split of the original data set will help us from going too far wrong.



In-class exercise
=========

- Download the `loudhou` data set and starter R script. Get a feel for how the code behaves:  
    1. Make a train/test split.  
    2. Train on the training set.  
    3. Predict on the testing set.  
    4. Plot the results.  
    

In-class exercise
=========    

- Then make an informal investigation of the _bias_ and _variance_ of the KNN estimator:   
    1. Repeatedly sample sets of size N=150 from the full data set, and train a $K=3$ model on each small training set.  Plot the fit to the training set.  How stable are they from sample to sample?  How do they behave at the endpoints, i.e. at very low and very high temperatures?  
    2. Now do the same thing, except training a $K=75$ model on each small training set of size $N=150$.  How stable are the estimates from sample to sample?  And how do they behave at the endpoints?   
- Hint: keep the x and y limits constant across plots, e.g. by adding the layers  `+ ylim(7000, 20000) + xlim(0,36)` or whatever limits seem appropriate.  

K=3: sample 1
=========    

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 3)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000) + xlim(0,36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

K=3: sample 2
=========    

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 3)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000) + xlim(0,36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

K=3: sample 3
=========    

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 3)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000) + xlim(0,36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

K=75: sample 1
=========    

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 75)

D_train$ypred = knn_model$pred 
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000) + xlim(0,36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

K=75: sample 2
=========    

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 74)

D_train$ypred = knn_model$pred 
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000) + xlim(0,36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```

K=75: sample 3
=========    

```{r, fig.width = 9, fig.height = 6, fig.align='center', echo=FALSE}
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,] 
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))

knn_model = knn.reg(X_train, X_train, y_train, k = 75)

D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) + 
  geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') + 
  theme_bw(base_size=18) + 
  ylim(7000, 20000) + xlim(0,36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
```



Bias-variance trade-off
=========  

- High K = high bias, low variance:    
    * We estimate $f(x)$ using many points, some of which might be far away from $x$. These far-away points bias the prediction; their values of $f(x)$ are slightly off on average.  
    * But more data points means lower variance---less chance of memorizing random noise.    

- Low K = low bias, high variance:    
    * We estimate $f(x)$ using only points that are _very close_ to $x$.  Far-away $x$ points don't bias the prediction with their "slightly off" $y$ values.  
    * But fewer data points means higher variance---more chance of memorizing random noise.    
    

Bias-variance trade-off
=========  

Let's take a deeper look at prediction error.  
- Let $\{(x_1, y_1), \ldots, (x_n, y_n)\}$ be your training data.  
- Suppose that $y = f(x) + e$, where $E(e) = 0$ and $\mbox{var}(e) = \sigma^2$.  
- Let $\hat{f}(x)$ be the function estimate arising from your training data.  
- Let $x^{\star}$ be some future $x$ point, and let $y^{\star}$ be the corresponding outcome.  
- $x^{\star}$ is fixed a priori, but the training data and the future outcome $y^{\star}$ are random.    

Define the expected squared prediction error at $x^{\star}$ as:  
$$
MSE^{\star} = E\left\{ \left( y^{\star} - \hat{f} (x^{\star}) \right)^2 \right\} 
$$



Bias-variance trade-off
=========  

For any random variable A, $E(A^2) = \mbox{var}(A) + E(A)^2$.  So:  

$$
\begin{aligned}
MSE^{\star} &= E\left\{ \left( y^{\star} - \hat{f} (x^{\star}) \right)^2 \right\} \\
  &= \mbox{var} \left\{ y^{\star} - \hat{f} (x^{\star}) \right\} + \left( E \left\{ y^{\star} - \hat{f} (x^{\star}) \right\} \right)^2 \\
  &= \mbox{var} \left\{ f(x^\star) + e^\star - \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) + e^\star - \hat{f} (x^{\star}) \right\} \right)^2 \\
  &= \sigma^2 + \mbox{var} \left\{ \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2 \\
  &= \sigma^2 + \mbox{(Estimation variance)} + \mbox{(Squared estimation bias)}
  \end{aligned}
$$


Bias-variance trade-off
=========  

$$
MSE^{\star} = \sigma^2 + \mbox{var} \left\{ \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2
$$

First, consider $\sigma^2$.  
- This is the intrinsic variability of the data: remember, $y = f(x) + e$, and $\mbox{var}(e) = \sigma^2$.  
- How can we make this term smaller?  


Bias-variance trade-off
=========  

$$
MSE^{\star} = \sigma^2 + \mbox{var} \left\{ \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2
$$

Next, consider $\mbox{var} \left\{ \hat{f} (x^{\star}) \right\}$.  
- This is the variance of our estimate $\hat{f}(x)$: remember, our estimate is random, because the data is random.  
- How can we make this term smaller?  


Bias-variance trade-off
=========  

$$
MSE^{\star} = \sigma^2 + \mbox{var} \left\{ \hat{f} (x^{\star}) \right\} + \left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2
$$

Finally, consider $\left( E \left\{ f(x^\star) - \hat{f} (x^{\star}) \right\} \right)^2$.  
- This is the bias of our estimate $\hat{f}(x)$: remember, our estimate doesn't necessarily equal the true $f(x)$, even on average.  
- How can we make this term smaller?  


Bias-variance trade-off
=========  

That's why it's a trade-off!  
- Smaller estimation variance generally requires a _less complex_ model---intuitively, one that "wiggles less" from sample to sample. (Think K=75 on our `loadhou` example.)
- Smaller bias generally requires a _more complex_ model---one that can "wiggle more," to adapt to the true function.  (Think K=3 on our `loadhou` example.)
- Models that "wiggle more" can adapt to more kinds of functions, but they're also more prone to memorizing random noise.  

__Much of the rest of the semester is about finding estimates with the right amount of wiggle!__


Classification
=========

_Classification_ is the term used when we want to predict a target variable $y$ that is categorical (win/lose, sick/healthy, pay/default, good/bad...).  For example, the plot below shows the longest run length of capital letters for a sample of spam and non-spam e-mails:

```{r, echo=FALSE, fig.width = 9, fig.height = 6, fig.align='center'}
spamfit = read.csv('spamfit.csv')
ggplot(data = spamfit) +
  theme_bw(base_size=18) +
  geom_boxplot(aes(factor(y), y=log(capital.run.length.average))) +
  labs(x="Spam?", y="Log(capital run length)")
```


Classification
=========

In this context, our approach is similar, but different in some specific ways.  If $y$ is binary (0/1), then our assumed model is:

$$
P(y = 1 \mid x) = f(x)
$$

In general, if $y \in \{1, \ldots, K\}$, then we have a _multi-class classification_ problem, and our goal is to estimate

$$
P(y = k \mid x) = f_k(x)
$$

for each category $k$.  Note there is an implicit constraint:

$$
\sum_{k=1}^K f_k(x) = 1
$$


Classification
=========

For example, in the spam classification problem, here is a linear estimate for $f(x)$.  It behaves somewhat reasonably, but also has some obviously undesirable features.

```{r, echo=FALSE, fig.width = 9, fig.height = 6, fig.align='center'}
lm1 = lm(y ~ log(capital.run.length.average), data=spamfit)
spamfit$lm1_pred = predict(lm1)
ggplot(data = spamfit) +
  theme_bw(base_size=18) +
  geom_point(aes(x=log(capital.run.length.average), y = jitter(y, 0.25)), color='grey', alpha=0.2) +
  labs(y="Spam?", x="Log(capital run length)") +
  geom_point(aes(x=log(capital.run.length.average), y = lm1_pred), color='blue', alpha=0.2)
```


Classification
=========

Here's a very different fit:

```{r, echo=FALSE, fig.width = 9, fig.height = 6, fig.align='center'}
lmsp = lm(y ~ splines::bs(log(capital.run.length.average), 12), data=spamfit)
spamfit$lm1_pred = predict(lmsp)
ggplot(data = spamfit) +
  theme_bw(base_size=18) +
  geom_point(aes(x=log(capital.run.length.average), y = jitter(y, 0.25)), color='grey', alpha=0.2) +
  labs(y="Spam?", x="Log(capital run length)") +
  geom_point(aes(x=log(capital.run.length.average), y = lm1_pred), color='blue', alpha=0.2) +
  ylim(-0.2,1.2)
```


Classification: from probabilities to predictions
=========

In binary classification, a very natural prediction rule is to threshold the probabilities:

$$
\hat{y} = \left\{
\begin{array}{ll}
1 & \mbox{if } f(x) \geq t \\
0 & \mbox{if } f(x) < t
\end{array}
\right.
$$

A natural choice is $t=0.5$, although this might not always be appropriate.

In multi-class problems, the extension of this idea is to predict using the "highest probability" class:

$$
\hat{y} = \arg_k \max f_k(x) \, .
$$


Classification: measuring accuracy
=========

What differs from numerical prediction is our _measure of accuracy_:  we are no longer dealing with a numerical outcome variable.

One common approach is to measure the accuracy of a model's predictions using the raw _error rate_ on the training sample:

$$
ER = \frac{1}{n} \sum_{i=1}^n I(y_i \neq \hat{y}_i)
$$

Here $I(\cdot)$ is the indicator function, taking the value 1 if its argument is true, and 0 otherwise.

Classification: out-of-sample error rate
=========

Just like with numerical prediction, we can define an out-of-sample version of the error rate:

$$
ER_{out} = \frac{1}{m} \sum_{i=1}^m I(y_i^{\star} \neq \hat{y}_i^{\star})
$$

where $(x_1^\star, y_1^\star), \ldots, (x_m^\star, y_m^\star)$ is a collection of $m$ "future" (out-of-sample) data points that weren't used to train the model.

As before, the practical way to estimate this quantity is to use a train/test split of the original data set.
